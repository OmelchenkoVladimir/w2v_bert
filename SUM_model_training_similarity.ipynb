{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import scipy\n",
    "import torch\n",
    "\n",
<<<<<<< HEAD
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "np.random.seed = 1"
=======
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
>>>>>>> 29978dd2bc87894cdd061a0c90b5201a5b194f4f
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('w2v_models/all_norm-sz500-w10-cb0-it3-min5.w2v', binary=True, unicode_errors='ignore')\n",
    "w2v.init_sims(replace=True) # Memory Error otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_medium_freq = pd.read_csv('rus/bert/bert_train.csv').sample(frac=1, random_state=1)\n",
    "bert_low_freq = pd.read_csv('rus/bert/bert_valid.csv').sample(frac=1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "(11577, 3)"
=======
       "(6251, 3)"
>>>>>>> 29978dd2bc87894cdd061a0c90b5201a5b194f4f
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_medium_freq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = bert_medium_freq[:(bert_medium_freq.shape[0] * 3 // 4)]\n",
<<<<<<< HEAD
    "valid = bert_medium_freq[(bert_medium_freq.shape[0] * 3 // 4):]\n",
    "\n",
    "small_valid=True\n",
    "if small_valid:\n",
    "    valid = valid.sample(frac=0.1, random_state=1)"
=======
    "valid = bert_medium_freq[(bert_medium_freq.shape[0] * 3 // 4):]"
>>>>>>> 29978dd2bc87894cdd061a0c90b5201a5b194f4f
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertToW2v(torch.nn.Module):\n",
    "    def __init__(self, bert_model_name, lin_shape_in, lin_shape_out, emb_layer): # -, 768, 100, 6\n",
    "        super(BertToW2v, self).__init__()\n",
    "        self.emb_layer = emb_layer\n",
    "        self.bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "        #self.bert_model.eval()\n",
    "        self.linear_model = torch.nn.Linear(lin_shape_in, lin_shape_out, bias=True) # bias?\n",
    "        torch.nn.init.uniform_(self.linear_model.weight, -0.1, 0.1)\n",
    "        \n",
    "    def forward(self, input_sentence): # ожидаем уже токенизированное предложение\n",
    "        encoded_layers, _ = self.bert_model(input_sentence)\n",
    "        bert_output = encoded_layers[self.emb_layer][0][1]\n",
    "        linear_output = self.linear_model(bert_output).unsqueeze(0)\n",
    "        return linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "bw2v = BertToW2v('bert-base-multilingual-cased', lin_shape_in=768, lin_shape_out=500, emb_layer=6) # !!!!!!\n",
=======
    "bw2v = BertToW2v('bert-base-multilingual-cased', lin_shape_in=768, lin_shape_out=500, emb_layer=6)\n",
>>>>>>> 29978dd2bc87894cdd061a0c90b5201a5b194f4f
    "bw2v.to('cuda');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladimir/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
<<<<<<< HEAD
      "  \"\"\"Entry point for launching an IPython kernel.\n"
=======
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/vladimir/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
>>>>>>> 29978dd2bc87894cdd061a0c90b5201a5b194f4f
     ]
    }
   ],
   "source": [
    "train['embedding'] = train['embedding'].apply(lambda x: ast.literal_eval(x))\n",
    "valid['embedding'] = valid['embedding'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "\n",
    "def get_embedding(word, defin, tokenizer, bw2v):\n",
    "    defin = '[CLS] [MASK] - ' + defin + ' [SEP]'\n",
    "    tok_def = tokenizer.tokenize(defin)\n",
    "    tok_ids = torch.tensor([tokenizer.convert_tokens_to_ids(tok_def)])\n",
    "    tok_ids = tok_ids.to('cuda')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        emb = bw2v(tok_ids)\n",
    "    \n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = pd.read_csv('rusfreq/freqrnc2011.csv', sep = '\\t')\n",
    "ozhegov_emb = pd.read_csv('rus/ozhegov/ozhegov_emb.csv')\n",
    "\n",
    "fd.drop_duplicates(subset=['Lemma'], keep=False, inplace=True)\n",
    "\n",
    "un_emb = set(ozhegov_emb['word'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "CPU times: user 424 ms, sys: 10.1 ms, total: 434 ms\n",
      "Wall time: 775 ms\n"
=======
      "CPU times: user 390 ms, sys: 8.7 ms, total: 399 ms\n",
      "Wall time: 978 ms\n"
>>>>>>> 29978dd2bc87894cdd061a0c90b5201a5b194f4f
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fd = fd[fd.apply(lambda row: row['Lemma'] in un_emb, axis=1)].sort_values(by='Freq(ipm)', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_unique = set(train['word'].unique())\n",
    "valid_unique = set(valid['word'].unique())\n",
    "test = bert_low_freq\n",
    "test_unique = set(test['word'].unique())\n",
    "\n",
    "fd_train = fd[fd.apply(lambda row: row['Lemma'] in train_unique, axis=1)] # 4658 rows\n",
    "fd_valid = fd[fd.apply(lambda row: row['Lemma'] in valid_unique, axis=1)] # 1549 rows\n",
    "fd_test = fd[fd.apply(lambda row: row['Lemma'] in test_unique, axis=1)]   # 21308 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "interval1 = 870 #450 \n",
    "interval2 = 290 #150\n",
    "interval3 = 850 #1000\n",
=======
    "interval1 = 450\n",
    "interval2 = 150\n",
    "interval3 = 1000\n",
>>>>>>> 29978dd2bc87894cdd061a0c90b5201a5b194f4f
    "\n",
    "sampled_train = fd_train.iloc[::interval1]\n",
    "sampled_valid = fd_valid.iloc[::interval2]\n",
    "\n",
    "sampled_test = fd_test.iloc[::interval3]\n",
    "\n",
    "def get_definition(word, ozhegov):\n",
    "    return ozhegov[ozhegov['word'] == word].iloc[0]['definition']\n",
    "\n",
    "def get_embedding(word, ozhegov):\n",
    "    return ozhegov[ozhegov['word'] == word].iloc[0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladimir/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/vladimir/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "for df in [sampled_train, sampled_valid, sampled_test]:\n",
    "    df['definition'] = df.apply(lambda row: get_definition(row['Lemma'], ozhegov_emb), axis=1)\n",
    "    df['embedding'] = df.apply(lambda row: get_embedding(row['Lemma'], ozhegov_emb), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladimir/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/vladimir/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "CPU times: user 1min 56s, sys: 1.11 s, total: 1min 57s\n",
      "Wall time: 21.9 s\n"
=======
      "CPU times: user 3min 56s, sys: 2.64 s, total: 3min 59s\n",
      "Wall time: 41.8 s\n"
>>>>>>> 29978dd2bc87894cdd061a0c90b5201a5b194f4f
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for df in [sampled_train, sampled_valid, sampled_test]:\n",
    "    df['similar_words'] = df.apply(lambda row: w2v.most_similar(row['Lemma']), axis=1)\n",
    "    \n",
    "for df in [sampled_train, sampled_valid, sampled_test]:\n",
    "    df['closest_word'] = df.apply(lambda row: w2v.most_similar(row['Lemma'], topn=1)[0][0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar(row, w2v, bw2v, tokenizer):\n",
    "    definition = '[CLS] [MASK] - ' + row.definition + ' [SEP]'\n",
    "    tok_def = tokenizer.tokenize(definition)\n",
    "    tok_ids = torch.tensor([tokenizer.convert_tokens_to_ids(tok_def)])\n",
    "    tok_ids = tok_ids.to('cuda')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding = bw2v(tok_ids).cpu().numpy()[0]\n",
    "        \n",
    "    return w2v.most_similar([embedding])\n",
    "\n",
    "def find_closest_position(row, w2v, bw2v, tokenizer):\n",
    "    definition = '[CLS] [MASK] - ' + row.definition + ' [SEP]'\n",
    "    tok_def = tokenizer.tokenize(definition)\n",
    "    tok_ids = torch.tensor([tokenizer.convert_tokens_to_ids(tok_def)])\n",
    "    tok_ids = tok_ids.to('cuda')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding = bw2v(tok_ids).cpu().numpy()[0]\n",
    "     \n",
    "    words = w2v.most_similar([embedding], topn=len(w2v.vocab)+1)\n",
    "    words = [x for (x, y) in words]\n",
    "    return words.index(row.Lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8682, 3)\n",
      "(290, 3)\n"
     ]
    }
   ],
   "source": [
    "n_epochs=50\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "loss_function = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(bw2v.parameters())#, lr=0.0001)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, patience=5)\n",
    "#small_valid = valid[:(train.shape[0] // 7)]\n",
    "print(train.shape)\n",
    "print(valid.shape)"
=======
   "outputs": [],
   "source": [
    "n_epochs=20\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "loss_function = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(bw2v.parameters(), lr=0.0001)\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer, n_epochs)\n",
    "#small_valid = valid[:(train.shape[0] // 7)]"
>>>>>>> 29978dd2bc87894cdd061a0c90b5201a5b194f4f
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "metadata": {
    "scrolled": true
   },
=======
   "metadata": {},
>>>>>>> 29978dd2bc87894cdd061a0c90b5201a5b194f4f
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "TRAIN_LOSS: 0.03530576647893478, VALID_LOSS: 0.030309630997864337\n"
=======
      "TRAIN_LOSS: 0.03193172446359505, VALID_LOSS: 0.025248441022897635\n"
>>>>>>> 29978dd2bc87894cdd061a0c90b5201a5b194f4f
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladimir/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/vladimir/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "TRAIN_CLOSEST: 5507045.1; VALID_CLOSEST: 5955149.0\n",
      "TRAIN_LOSS: 0.0357970347759082, VALID_LOSS: 0.030645329065235524\n",
      "TRAIN_CLOSEST: 5387455.5; VALID_CLOSEST: 6521911.0\n",
      "TRAIN_LOSS: 0.030181049515485277, VALID_LOSS: 0.03009859286248684\n",
      "TRAIN_CLOSEST: 5506450.9; VALID_CLOSEST: 6331810.0\n",
      "TRAIN_LOSS: 0.02996616644661018, VALID_LOSS: 0.030076203019968394\n",
      "TRAIN_CLOSEST: 5522541.8; VALID_CLOSEST: 6357902.0\n",
      "TRAIN_LOSS: 0.02996069789090488, VALID_LOSS: 0.030057420125552292\n",
      "TRAIN_CLOSEST: 5501709.1; VALID_CLOSEST: 6366503.0\n",
      "TRAIN_LOSS: 0.029959194710417258, VALID_LOSS: 0.03008960768708895\n",
      "TRAIN_CLOSEST: 5492444.8; VALID_CLOSEST: 6356214.0\n",
      "TRAIN_LOSS: 0.029959161828680317, VALID_LOSS: 0.030166333833516672\n",
      "TRAIN_CLOSEST: 5495892.7; VALID_CLOSEST: 6206141.0\n",
      "TRAIN_LOSS: 0.02996102212675077, VALID_LOSS: 0.030093287397176028\n",
      "TRAIN_CLOSEST: 5479591.2; VALID_CLOSEST: 6426542.0\n",
      "TRAIN_LOSS: 0.029961874810333, VALID_LOSS: 0.03010733775666048\n",
      "TRAIN_CLOSEST: 5492352.5; VALID_CLOSEST: 6274370.0\n",
      "TRAIN_LOSS: 0.02996457842045842, VALID_LOSS: 0.0301058785732964\n",
      "TRAIN_CLOSEST: 5511190.9; VALID_CLOSEST: 6248215.0\n",
      "TRAIN_LOSS: 0.02996578866625975, VALID_LOSS: 0.03013156343829529\n",
      "TRAIN_CLOSEST: 5480039.9; VALID_CLOSEST: 6289611.0\n",
      "TRAIN_LOSS: 0.0297236289729576, VALID_LOSS: 0.029808061081787636\n",
      "TRAIN_CLOSEST: 5617863.3; VALID_CLOSEST: 6611700.0\n",
      "TRAIN_LOSS: 0.029711932120889774, VALID_LOSS: 0.029804021081533925\n",
      "TRAIN_CLOSEST: 5620471.5; VALID_CLOSEST: 6577047.0\n",
      "TRAIN_LOSS: 0.029706412441882636, VALID_LOSS: 0.02979589128096042\n",
      "TRAIN_CLOSEST: 5611630.9; VALID_CLOSEST: 6575417.0\n",
      "TRAIN_LOSS: 0.029701993965574484, VALID_LOSS: 0.029789356344604286\n",
      "TRAIN_CLOSEST: 5620161.3; VALID_CLOSEST: 6567441.0\n",
      "TRAIN_LOSS: 0.0296992735220778, VALID_LOSS: 0.029781596747965648\n",
      "TRAIN_CLOSEST: 5617210.3; VALID_CLOSEST: 6553709.0\n",
      "TRAIN_LOSS: 0.029696314005866437, VALID_LOSS: 0.02977156126550559\n",
      "TRAIN_CLOSEST: 5605964.9; VALID_CLOSEST: 6581753.0\n",
      "TRAIN_LOSS: 0.029694497841029528, VALID_LOSS: 0.029765619176985888\n",
      "TRAIN_CLOSEST: 5608785.4; VALID_CLOSEST: 6569273.0\n",
      "TRAIN_LOSS: 0.029692953218189124, VALID_LOSS: 0.029759832205058172\n",
      "TRAIN_CLOSEST: 5610022.8; VALID_CLOSEST: 6594247.0\n",
      "TRAIN_LOSS: 0.029691572492245813, VALID_LOSS: 0.029756485050993747\n",
      "TRAIN_CLOSEST: 5605532.4; VALID_CLOSEST: 6601236.0\n",
      "TRAIN_LOSS: 0.02969061690089811, VALID_LOSS: 0.029755710023615895\n",
      "TRAIN_CLOSEST: 5608135.0; VALID_CLOSEST: 6599441.0\n",
      "TRAIN_LOSS: 0.029689773937847753, VALID_LOSS: 0.029753668442496967\n",
      "TRAIN_CLOSEST: 5606407.7; VALID_CLOSEST: 6587865.0\n",
      "TRAIN_LOSS: 0.029689121328893276, VALID_LOSS: 0.029758260778055108\n",
      "TRAIN_CLOSEST: 5615701.4; VALID_CLOSEST: 6590531.0\n",
      "TRAIN_LOSS: 0.029688132013198767, VALID_LOSS: 0.029759742281046407\n",
      "TRAIN_CLOSEST: 5628187.3; VALID_CLOSEST: 6589215.0\n",
      "TRAIN_LOSS: 0.02968765322902716, VALID_LOSS: 0.029755363064207906\n",
      "TRAIN_CLOSEST: 5620006.3; VALID_CLOSEST: 6580791.0\n",
      "TRAIN_LOSS: 0.029687008800629102, VALID_LOSS: 0.02975402309013338\n",
      "TRAIN_CLOSEST: 5622939.2; VALID_CLOSEST: 6568795.0\n",
      "TRAIN_LOSS: 0.029692963148381213, VALID_LOSS: 0.029696665643232648\n",
      "TRAIN_CLOSEST: 5624846.4; VALID_CLOSEST: 6704983.0\n",
      "TRAIN_LOSS: 0.029688452590959338, VALID_LOSS: 0.029695173847521173\n",
      "TRAIN_CLOSEST: 5630790.3; VALID_CLOSEST: 6705717.0\n",
      "TRAIN_LOSS: 0.02968806276906499, VALID_LOSS: 0.02969508458828104\n",
      "TRAIN_CLOSEST: 5631800.6; VALID_CLOSEST: 6709109.0\n",
      "TRAIN_LOSS: 0.02968782067990355, VALID_LOSS: 0.029694694694902362\n",
      "TRAIN_CLOSEST: 5630391.4; VALID_CLOSEST: 6716915.0\n",
      "TRAIN_LOSS: 0.02968783007218766, VALID_LOSS: 0.02969500419266265\n",
      "TRAIN_CLOSEST: 5630908.8; VALID_CLOSEST: 6714324.0\n",
      "TRAIN_LOSS: 0.029688085314483685, VALID_LOSS: 0.029694707409061234\n",
      "TRAIN_CLOSEST: 5628018.4; VALID_CLOSEST: 6706497.0\n",
      "TRAIN_LOSS: 0.029687787086158594, VALID_LOSS: 0.029693849458648214\n",
      "TRAIN_CLOSEST: 5630355.5; VALID_CLOSEST: 6701416.0\n",
      "TRAIN_LOSS: 0.029692030434454435, VALID_LOSS: 0.02969006890238359\n",
      "TRAIN_CLOSEST: 5627784.6; VALID_CLOSEST: 6739742.0\n",
      "TRAIN_LOSS: 0.029689724562572524, VALID_LOSS: 0.02968799633712604\n",
      "TRAIN_CLOSEST: 5619939.5; VALID_CLOSEST: 6759248.0\n",
      "TRAIN_LOSS: 0.029689113759403318, VALID_LOSS: 0.029687732670070795\n",
      "TRAIN_CLOSEST: 5624988.3; VALID_CLOSEST: 6758350.0\n",
      "TRAIN_LOSS: 0.02968873692362337, VALID_LOSS: 0.02968713508212361\n",
      "TRAIN_CLOSEST: 5624817.7; VALID_CLOSEST: 6771124.0\n",
      "TRAIN_LOSS: 0.02968872497706749, VALID_LOSS: 0.029687094010798068\n",
      "TRAIN_CLOSEST: 5624255.1; VALID_CLOSEST: 6765081.0\n",
      "TRAIN_LOSS: 0.029688436225931116, VALID_LOSS: 0.02968692316448894\n",
      "TRAIN_CLOSEST: 5620608.9; VALID_CLOSEST: 6765071.0\n",
      "TRAIN_LOSS: 0.02968835595819693, VALID_LOSS: 0.029686895202331504\n",
      "TRAIN_CLOSEST: 5621440.2; VALID_CLOSEST: 6770772.0\n"
=======
      "Validation loss has improved; saving model\n",
      "TRAIN_LOSS: 0.024172643839151985, VALID_LOSS: 0.0234919100641127\n",
      "TRAIN_LOSS: 0.022444200674993694, VALID_LOSS: 0.02292396454408679\n",
      "Validation loss has improved; saving model\n",
      "TRAIN_LOSS: 0.021179002214027663, VALID_LOSS: 0.022698503930862906\n",
      "Validation loss has improved; saving model\n",
      "TRAIN_LOSS: 0.020198152930679995, VALID_LOSS: 0.02254213071925779\n",
      "Validation loss has improved; saving model\n",
      "TRAIN_LOSS: 0.019268891653530743, VALID_LOSS: 0.022557125832308716\n",
      "TRAIN_LOSS: 0.018325413839888373, VALID_LOSS: 0.022503364609550835\n",
      "Validation loss has improved; saving model\n",
      "TRAIN_LOSS: 0.017416365288065686, VALID_LOSS: 0.022694245741273003\n",
      "TRAIN_LOSS: 0.01644134535663801, VALID_LOSS: 0.022650450232104467\n",
      "TRAIN_LOSS: 0.015452735706406515, VALID_LOSS: 0.02273568723863795\n",
      "TRAIN_LOSS: 0.01450482289945117, VALID_LOSS: 0.02314174665338929\n",
      "TRAIN_LOSS: 0.013515988132630717, VALID_LOSS: 0.023216954433321668\n"
>>>>>>> 29978dd2bc87894cdd061a0c90b5201a5b194f4f
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "# best_val_loss = 1\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    losses = []\n",
    "    for row in train.itertuples():\n",
    "        \n",
    "        defin = row.definition\n",
    "        defin = '[CLS] [MASK] - ' + defin + ' [SEP]'\n",
    "        tok_def = tokenizer.tokenize(defin)\n",
    "        tok_ids = torch.tensor([tokenizer.convert_tokens_to_ids(tok_def)])\n",
    "        tok_ids = tok_ids.to('cuda')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        linear_output = bw2v(tok_ids)\n",
    "\n",
    "        #tensor_ones = torch.ones(1).to('cuda')\n",
    "        y = torch.tensor(row.embedding).unsqueeze(0).to('cuda')\n",
    "        loss = loss_function(linear_output, y)#, tensor_ones)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(float(loss.cpu()))\n",
    "    \n",
    "    valid_losses = []\n",
    "    \n",
    "    for row in valid.itertuples():\n",
    "        with torch.no_grad():\n",
    "            defin = row.definition\n",
    "            defin = '[CLS] [MASK] ' + defin + ' [SEP]'\n",
    "            tok_def = tokenizer.tokenize(defin)\n",
    "            tok_ids = torch.tensor([tokenizer.convert_tokens_to_ids(tok_def)])\n",
    "            tok_ids = tok_ids.to('cuda')\n",
    "            \n",
    "            linear_output = bw2v(tok_ids)\n",
    "            \n",
    "            y = torch.tensor(row.embedding).unsqueeze(0).to('cuda')\n",
    "            loss = loss_function(linear_output, y)\n",
    "            \n",
    "            valid_losses.append(float(loss.cpu()))\n",
    "        \n",
    "    print('TRAIN_LOSS: {0}, VALID_LOSS: {1}'.format((sum(losses) / len(losses)), (sum(valid_losses) / len(valid_losses))))\n",
    "    \n",
    "    for df in [sampled_train, sampled_valid, sampled_test]:\n",
    "        df[f'similar_words_epoch_{i}'] = df.apply(lambda row: find_similar(row, w2v, bw2v, tokenizer), axis=1)\n",
    "        df[f'closest_word_epoch_{i}'] = df.apply(lambda row: find_closest_position(row, w2v, bw2v, tokenizer), axis=1)\n",
<<<<<<< HEAD
    "        if (df is sampled_train):\n",
    "            print('TRAIN_CLOSEST: ' + str(df[f'closest_word_epoch_{i}'].mean()), end='')\n",
    "        if (df is sampled_valid):\n",
    "            print('; VALID_CLOSEST: ' + str(df[f'closest_word_epoch_{i}'].mean()))\n",
    "        \n",
    "    scheduler.step(sum(valid_losses) / len(valid_losses))\n",
    "    \n",
    "    # saving best model (by val_loss)\n",
    "    if (i == 0):\n",
    "        torch.save(bw2v.state_dict(), f'models/v{i}ep_l6.mdl')\n",
=======
    "        \n",
    "    scheduler.step()\n",
    "    \n",
    "    # saving best model (by val_loss)\n",
    "    if (i == 0):\n",
    "        torch.save(bw2v.state_dict(), f'models/SUM_cosine_annealing_v{i}ep.mdl')\n",
>>>>>>> 29978dd2bc87894cdd061a0c90b5201a5b194f4f
    "        best_val_loss = (sum(valid_losses) / len(valid_losses))\n",
    "    else:\n",
    "        if (sum(valid_losses) / len(valid_losses)) <= best_val_loss:\n",
    "            best_val_loss = (sum(valid_losses) / len(valid_losses))\n",
<<<<<<< HEAD
    "            #print('Validation loss has improved; saving model')\n",
    "        torch.save(bw2v.state_dict(), f'models/v{i}ep_l6.mdl')"
=======
    "            print('Validation loss has improved; saving model')\n",
    "            torch.save(bw2v.state_dict(), f'models/SUM_cosine_annealing_v{i}ep.mdl')"
>>>>>>> 29978dd2bc87894cdd061a0c90b5201a5b194f4f
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "torch.save(bw2v.state_dict(), f'models/v{n_epochs}ep_l6.mdl')\n",
    "\n",
    "sampled_train.to_csv(f'rus/bert/sampled_train_v{n_epochs}ep_l6.csv', index=None)\n",
    "sampled_valid.to_csv(f'rus/bert/sampled_valid_v{n_epochs}ep_l6.csv', index=None)\n",
    "sampled_test.to_csv(f'rus/bert/sampled_test_v{n_epochs}ep_l6.csv', index=None)"
=======
    "torch.save(bw2v.state_dict(), f'models/SUM_cosine_annealing_v{n_epochs}ep.mdl')\n",
    "\n",
    "sampled_train.to_csv(f'rus/bert/sampled_train_v{n_epochs}ep.csv', index=None)\n",
    "sampled_valid.to_csv(f'rus/bert/sampled_valid_v{n_epochs}ep.csv', index=None)\n",
    "sampled_test.to_csv(f'rus/bert/sampled_test_v{n_epochs}ep.csv', index=None)"
>>>>>>> 29978dd2bc87894cdd061a0c90b5201a5b194f4f
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampled_valid[[x for x in list(sampled_train.columns) if x.startswith('closest')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampled_train"
=======
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_train[[x for x in list(sampled_train.columns) if x.startswith('closest')]]"
>>>>>>> 29978dd2bc87894cdd061a0c90b5201a5b194f4f
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "test.shape"
=======
    "sampled_test"
>>>>>>> 29978dd2bc87894cdd061a0c90b5201a5b194f4f
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
