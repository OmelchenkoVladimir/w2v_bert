{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pytorch_transformers import BertTokenizer, BertModel, BertForMaskedLM, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "np.random.seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefinitionsDataset(Dataset): # train_maxlen = 103; set_to_128\n",
    "    \n",
    "    \n",
    "    def __init__(self, path_to_csv, tokenizer, max_len=128):\n",
    "        self.data = pd.read_csv(path_to_csv)\n",
    "        self.data['input'] = self.data['definition'].apply(lambda x: torch.tensor(tokenizer.convert_tokens_to_ids(tokenizer.tokenize('[CLS] [MASK] - ' + x + ' [SEP]'))))\n",
    "        self.data['embedding'] = self.data['embedding'].apply(lambda x: ast.literal_eval(x))\n",
    "        self.data['label'] = self.data['embedding'].apply(lambda x: torch.tensor(x))\n",
    "        self.data['input'] = self.data['input'].apply(lambda x: F.pad(input=x, pad=(0, max_len-(x.shape[0])), mode='constant', value=0))\n",
    "        self.data['attention'] = self.data['input'].apply(lambda x: torch.tensor([float(el>0) for el in x]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "        return {'input': self.data.iloc[index]['input'], 'attention_mask':self.data.iloc[index]['attention'], 'label': self.data.iloc[index]['label']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DefinitionsDataset('rus/bert/bert_train.csv', tokenizer)\n",
    "valid = DefinitionsDataset('rus/bert/bert_valid.csv', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertToW2v(torch.nn.Module):\n",
    "    def __init__(self, bert_model_name, lin_shape_in, lin_shape_out, emb_layer): # -, 768, 100, 6\n",
    "        super(BertToW2v, self).__init__()\n",
    "        self.emb_layer = emb_layer\n",
    "        self.bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "        #self.bert_model.eval()\n",
    "        self.linear_model = torch.nn.Linear(lin_shape_in, lin_shape_out, bias=True) # bias?\n",
    "        torch.nn.init.uniform_(self.linear_model.weight, -0.1, 0.1)\n",
    "        \n",
    "    def forward(self, input_sentence, mask): # ожидаем уже токенизированное предложение\n",
    "        encoded_layers, _ = self.bert_model(input_sentence, attention_mask=mask)\n",
    "        bert_output = encoded_layers[:,self.emb_layer]\n",
    "        linear_output = self.linear_model(bert_output)\n",
    "        return linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw2v = BertToW2v('bert-base-multilingual-cased', lin_shape_in=768, lin_shape_out=500, emb_layer=6) # !\n",
    "bw2v.to('cuda');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dl = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(bw2v.parameters())\n",
    "loss_function = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING_LOSS: 0.04628171315083827; VALIDATION_LOSS: 0.031234915636878764\n",
      "TRAINING_LOSS: 0.03202891845150297; VALIDATION_LOSS: 0.03099616715726354\n",
      "TRAINING_LOSS: 0.03131647099156627; VALIDATION_LOSS: 0.03079848895122865\n",
      "TRAINING_LOSS: 0.030943178694348088; VALIDATION_LOSS: 0.030661838405825106\n",
      "TRAINING_LOSS: 0.0305505803752127; VALIDATION_LOSS: 0.030604709666833565\n",
      "TRAINING_LOSS: 0.030430804777980148; VALIDATION_LOSS: 0.030553429858800867\n",
      "TRAINING_LOSS: 0.030306716916275945; VALIDATION_LOSS: 0.030436578645838975\n",
      "TRAINING_LOSS: 0.030301890306310718; VALIDATION_LOSS: 0.030421839187336708\n",
      "TRAINING_LOSS: 0.030186987705766066; VALIDATION_LOSS: 0.030391465930432234\n",
      "TRAINING_LOSS: 0.030083640303268454; VALIDATION_LOSS: 0.030345965971420672\n",
      "TRAINING_LOSS: 0.030052811139264318; VALIDATION_LOSS: 0.03032702065695444\n",
      "TRAINING_LOSS: 0.029998990668985707; VALIDATION_LOSS: 0.030289445490622974\n",
      "TRAINING_LOSS: 0.029972133539949326; VALIDATION_LOSS: 0.0302778928056488\n",
      "TRAINING_LOSS: 0.02994098867539221; VALIDATION_LOSS: 0.030230849125584994\n",
      "TRAINING_LOSS: 0.02991050750583542; VALIDATION_LOSS: 0.03022114322593175\n",
      "TRAINING_LOSS: 0.029901716226607082; VALIDATION_LOSS: 0.0302233342628199\n",
      "TRAINING_LOSS: 0.029876861873732157; VALIDATION_LOSS: 0.03020560563202555\n",
      "TRAINING_LOSS: 0.029857409685156655; VALIDATION_LOSS: 0.030215198103344915\n",
      "TRAINING_LOSS: 0.029850668590743125; VALIDATION_LOSS: 0.030190077586186366\n",
      "TRAINING_LOSS: 0.02983341990977014; VALIDATION_LOSS: 0.030175143228731213\n",
      "CPU times: user 32min 51s, sys: 9min 32s, total: 42min 24s\n",
      "Wall time: 42min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    bw2v.train()\n",
    "    train_loss = 0.0\n",
    "    for dct in train_dl:\n",
    "        inputs = dct['input']\n",
    "        masks = dct['attention_mask']\n",
    "        labels = dct['label']\n",
    "        inputs = inputs.to('cuda')\n",
    "        masks = masks.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = bw2v(inputs, masks)\n",
    "        loss = loss_function(output, labels)\n",
    "        \n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('TRAINING_LOSS: ', end='')\n",
    "    print(train_loss / len(train_dl.dataset), end='')\n",
    "    \n",
    "    bw2v.eval()\n",
    "    valid_loss = 0.0\n",
    "    for dct in valid_dl:\n",
    "        inputs = dct['input']\n",
    "        masks = dct['attention_mask']\n",
    "        labels = dct['label']\n",
    "        inputs = inputs.to('cuda')\n",
    "        masks = masks.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = bw2v(inputs, masks)\n",
    "            loss = loss_function(output, labels)\n",
    "        \n",
    "        valid_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    print('; VALIDATION_LOSS: ', end='')\n",
    "    print(valid_loss / len(valid_dl.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(bw2v.state_dict(), f'models/batchify_ep20_l6.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
